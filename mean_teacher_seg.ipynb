{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from data_loader import data_load\n",
    "from unet_pytorch import build_unet\n",
    "from ramps import get_current_consistency_weight, update_ema_variables\n",
    "from glob import glob\n",
    "import tensorflow as tf\n",
    "from time import time\n",
    "from datetime import datetime\n",
    "from monai.data import decollate_batch\n",
    "from monai.losses import DiceLoss\n",
    "from monai.metrics import DiceMetric\n",
    "from monai.transforms import (\n",
    "    Compose,\n",
    "    AsDiscrete,\n",
    "    EnsureType,\n",
    ")\n",
    "import os\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "writer = SummaryWriter()\n",
    "\n",
    "\n",
    "def load_dataset(batch, folder, label=True, buffer_size=1000):\n",
    "    features = BANDS + TARGETS if label else BANDS\n",
    "    tf_files = glob(f\"{folder}/*.gz\")\n",
    "    columns = [\n",
    "        tf.io.FixedLenFeature(\n",
    "            shape=KERNEL_SHAPE if label else BUFFERED_SHAPE, dtype=tf.float32\n",
    "        )\n",
    "        for _feature in features\n",
    "    ]\n",
    "    description = dict(zip(features, columns))\n",
    "    data_func = data_load(\n",
    "        tf_files,\n",
    "        BANDS,\n",
    "        description,\n",
    "        response=TARGETS,\n",
    "        batch_size=batch,\n",
    "        buffer_size=buffer_size,\n",
    "    )\n",
    "    data = (\n",
    "        data_func.get_training_dataset()\n",
    "        if label\n",
    "        else data_func.get_pridiction_dataset()\n",
    "    )\n",
    "    return data\n",
    "\n",
    "BANDS = [\"blue\", \"green\", \"red\", \"nir\", \"swir1\", \"swir2\", \"ndvi\", \"nirv\"]\n",
    "KERNEL_SHAPE = [256, 256]\n",
    "KERNEL_BUFFER = [128, 128]\n",
    "X_BUFFER, Y_BUFFER = [buffer // 2 for buffer in KERNEL_BUFFER]\n",
    "X_BUFFERED, Y_BUFFERED = (X_BUFFER + KERNEL_SHAPE[0]), (Y_BUFFER + KERNEL_SHAPE[1])\n",
    "BUFFERED_SHAPE = [\n",
    "    kernel + buffer for kernel, buffer in zip(KERNEL_SHAPE, KERNEL_BUFFER)\n",
    "]\n",
    "TARGETS = [\"cropland\"]\n",
    "NCLASS = 2\n",
    "model_folder = f\"/bess23/huaize/semi-supervised/models/\"\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "run_time = datetime.today().strftime(\"%m_%d_%H_%M_%S\")\n",
    "batch = 256\n",
    "\n",
    "\n",
    "dataset = load_dataset(\n",
    "    batch, \"/bess23/huaize/semi-supervised/data/labeled/train\", label=True\n",
    ")\n",
    "dataset_size = dataset.reduce(0, lambda x, _: x + 1).numpy()\n",
    "# val_dataset = load_dataset(\n",
    "#     batch, \"/bess23/huaize/semi-supervised/data/labeled/valid\", label=True\n",
    "# )\n",
    "validation_size = int(0.3 * dataset_size)  # 0.3\n",
    "train_dataset = dataset.take(validation_size).batch(batch)\n",
    "validation_dataset = dataset.skip(validation_size).batch(batch)\n",
    "test_dataset = load_dataset(\n",
    "    batch, \"/bess23/huaize/semi-supervised/data/unlabeled\", label=False\n",
    ")\n",
    "model = build_unet(len(BANDS), NCLASS).cuda()\n",
    "ema_model = build_unet(len(BANDS), NCLASS).cuda()\n",
    "model = nn.DataParallel(model)\n",
    "ema_model = nn.DataParallel(ema_model)\n",
    "model.to(device)\n",
    "ema_model.to(device)\n",
    "\n",
    "lr = 3e-4\n",
    "opt = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "loss_function = DiceLoss(to_onehot_y=True, softmax=True)\n",
    "dice_metric = DiceMetric(include_background=False, reduction=\"mean\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------\n",
      "epoch 1/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/bess23/huaize/semi-supervised/meanteacherseg/ramps.py:54: UserWarning: This overload of add_ is deprecated:\n",
      "\tadd_(Number alpha, Tensor other)\n",
      "Consider using one of the following signatures instead:\n",
      "\tadd_(Tensor other, *, Number alpha) (Triggered internally at ../torch/csrc/utils/python_arg_parser.cpp:1420.)\n",
      "  ema_param.data.mul_(alpha).add_(1 - alpha, param.data)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss: 0.6300\n",
      "epoch 1 average loss: 0.6300\n",
      "epoch time = 22.118485927581787\n",
      "----------\n",
      "epoch 2/300\n",
      "train_loss: 0.6232\n",
      "epoch 2 average loss: 0.6232\n",
      "epoch time = 16.529382705688477\n",
      "----------\n",
      "epoch 3/300\n",
      "train_loss: 0.6180\n",
      "epoch 3 average loss: 0.6180\n",
      "val dice: 0.0\n",
      "saved new best metric model\n",
      "current epoch: 3 current mean dice: 0.0000\n",
      "best mean dice: 0.0000 at epoch: 3\n",
      "epoch time = 26.10469889640808\n",
      "----------\n",
      "epoch 4/300\n",
      "train_loss: 0.6179\n",
      "epoch 4 average loss: 0.6179\n",
      "epoch time = 16.458182334899902\n",
      "----------\n",
      "epoch 5/300\n",
      "train_loss: 0.6062\n",
      "epoch 5 average loss: 0.6062\n",
      "epoch time = 16.439297199249268\n",
      "----------\n",
      "epoch 6/300\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 23\u001b[0m\n\u001b[1;32m     21\u001b[0m val_loader \u001b[39m=\u001b[39m validation_dataset\u001b[39m.\u001b[39mas_numpy_iterator()\n\u001b[1;32m     22\u001b[0m unlabeled_train_loader \u001b[39m=\u001b[39m test_dataset\u001b[39m.\u001b[39mas_numpy_iterator()\n\u001b[0;32m---> 23\u001b[0m \u001b[39mfor\u001b[39;00m labeled_batch, unlabeled_batch \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(train_loader, unlabeled_train_loader):\n\u001b[1;32m     24\u001b[0m     step \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m     25\u001b[0m     labeled_inputs, labels \u001b[39m=\u001b[39m (\n\u001b[1;32m     26\u001b[0m         torch\u001b[39m.\u001b[39mtensor(labeled_batch[\u001b[39m0\u001b[39m])\u001b[39m.\u001b[39mto(device),\n\u001b[1;32m     27\u001b[0m         torch\u001b[39m.\u001b[39mtensor(labeled_batch[\u001b[39m1\u001b[39m])\u001b[39m.\u001b[39mto(device),\n\u001b[1;32m     28\u001b[0m     )\n",
      "File \u001b[0;32m~/semi-supervised/pytorch/lib/python3.8/site-packages/tensorflow/python/data/ops/dataset_ops.py:4770\u001b[0m, in \u001b[0;36m_NumpyIterator.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   4767\u001b[0m     numpy\u001b[39m.\u001b[39msetflags(write\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m   4768\u001b[0m   \u001b[39mreturn\u001b[39;00m numpy\n\u001b[0;32m-> 4770\u001b[0m \u001b[39mreturn\u001b[39;00m nest\u001b[39m.\u001b[39mmap_structure(to_numpy, \u001b[39mnext\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_iterator))\n",
      "File \u001b[0;32m~/semi-supervised/pytorch/lib/python3.8/site-packages/tensorflow/python/data/ops/iterator_ops.py:787\u001b[0m, in \u001b[0;36mOwnedIterator.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    785\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__next__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    786\u001b[0m   \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 787\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_internal()\n\u001b[1;32m    788\u001b[0m   \u001b[39mexcept\u001b[39;00m errors\u001b[39m.\u001b[39mOutOfRangeError:\n\u001b[1;32m    789\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m\n",
      "File \u001b[0;32m~/semi-supervised/pytorch/lib/python3.8/site-packages/tensorflow/python/data/ops/iterator_ops.py:770\u001b[0m, in \u001b[0;36mOwnedIterator._next_internal\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    767\u001b[0m \u001b[39m# TODO(b/77291417): This runs in sync mode as iterators use an error status\u001b[39;00m\n\u001b[1;32m    768\u001b[0m \u001b[39m# to communicate that there is no more data to iterate over.\u001b[39;00m\n\u001b[1;32m    769\u001b[0m \u001b[39mwith\u001b[39;00m context\u001b[39m.\u001b[39mexecution_mode(context\u001b[39m.\u001b[39mSYNC):\n\u001b[0;32m--> 770\u001b[0m   ret \u001b[39m=\u001b[39m gen_dataset_ops\u001b[39m.\u001b[39;49miterator_get_next(\n\u001b[1;32m    771\u001b[0m       \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_iterator_resource,\n\u001b[1;32m    772\u001b[0m       output_types\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_flat_output_types,\n\u001b[1;32m    773\u001b[0m       output_shapes\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_flat_output_shapes)\n\u001b[1;32m    775\u001b[0m   \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    776\u001b[0m     \u001b[39m# Fast path for the case `self._structure` is not a nested structure.\u001b[39;00m\n\u001b[1;32m    777\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_element_spec\u001b[39m.\u001b[39m_from_compatible_tensor_list(ret)  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n",
      "File \u001b[0;32m~/semi-supervised/pytorch/lib/python3.8/site-packages/tensorflow/python/ops/gen_dataset_ops.py:3012\u001b[0m, in \u001b[0;36miterator_get_next\u001b[0;34m(iterator, output_types, output_shapes, name)\u001b[0m\n\u001b[1;32m   3010\u001b[0m \u001b[39mif\u001b[39;00m tld\u001b[39m.\u001b[39mis_eager:\n\u001b[1;32m   3011\u001b[0m   \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 3012\u001b[0m     _result \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_FastPathExecute(\n\u001b[1;32m   3013\u001b[0m       _ctx, \u001b[39m\"\u001b[39;49m\u001b[39mIteratorGetNext\u001b[39;49m\u001b[39m\"\u001b[39;49m, name, iterator, \u001b[39m\"\u001b[39;49m\u001b[39moutput_types\u001b[39;49m\u001b[39m\"\u001b[39;49m, output_types,\n\u001b[1;32m   3014\u001b[0m       \u001b[39m\"\u001b[39;49m\u001b[39moutput_shapes\u001b[39;49m\u001b[39m\"\u001b[39;49m, output_shapes)\n\u001b[1;32m   3015\u001b[0m     \u001b[39mreturn\u001b[39;00m _result\n\u001b[1;32m   3016\u001b[0m   \u001b[39mexcept\u001b[39;00m _core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# %% train\n",
    "max_epochs = 300\n",
    "MeanTeacherEpoch = 50\n",
    "val_interval = 3\n",
    "best_metric = -1\n",
    "best_metric_epoch = -1\n",
    "iter_num = 0\n",
    "epoch_loss_values = []\n",
    "metric_values = []\n",
    "post_pred = Compose([EnsureType(), AsDiscrete(argmax=True, to_onehot=NCLASS)])\n",
    "post_label = Compose([EnsureType(), AsDiscrete(to_onehot=NCLASS)])\n",
    "\n",
    "for epoch in range(max_epochs):\n",
    "    print(\"-\" * 10)\n",
    "    print(f\"epoch {epoch + 1}/{max_epochs}\")\n",
    "    start_time = time()\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    step = 0\n",
    "    train_loader = train_dataset.as_numpy_iterator()\n",
    "    val_loader = validation_dataset.as_numpy_iterator()\n",
    "    unlabeled_train_loader = test_dataset.as_numpy_iterator()\n",
    "    for labeled_batch, unlabeled_batch in zip(train_loader, unlabeled_train_loader):\n",
    "        step += 1\n",
    "        labeled_inputs, labels = (\n",
    "            torch.tensor(labeled_batch[0]).to(device),\n",
    "            torch.tensor(labeled_batch[1]).to(device),\n",
    "        )\n",
    "        unlabeled_batch = unlabeled_batch[\n",
    "            slice(None), slice(None), X_BUFFER:X_BUFFERED, Y_BUFFER:Y_BUFFERED\n",
    "        ]\n",
    "        unlabeled_inputs = torch.tensor(unlabeled_batch).to(device)\n",
    "        opt.zero_grad()\n",
    "        noise_labeled = torch.clamp(torch.randn_like(labeled_inputs) * 0.1, -0.2, 0.2)\n",
    "        noise_unlabeled = torch.clamp(\n",
    "            torch.randn_like(unlabeled_inputs) * 0.1, -0.2, 0.2\n",
    "        )\n",
    "        noise_labeled_inputs = labeled_inputs + noise_labeled\n",
    "        noise_unlabeled_inputs = unlabeled_inputs + noise_unlabeled\n",
    "\n",
    "        outputs = model(labeled_inputs)\n",
    "        with torch.no_grad():\n",
    "            soft_out = torch.softmax(outputs, dim=1)\n",
    "            outputs_unlabeled = model(unlabeled_inputs)\n",
    "            soft_unlabeled = torch.softmax(outputs_unlabeled, dim=1)\n",
    "            outputs_aug = ema_model(noise_labeled_inputs)\n",
    "            soft_aug = torch.softmax(outputs_aug, dim=1)\n",
    "            outputs_unlabeled_aug = ema_model(noise_unlabeled_inputs)\n",
    "            soft_unlabeled_aug = torch.softmax(outputs_unlabeled_aug, dim=1)\n",
    "\n",
    "        supervised_loss = loss_function(outputs, labels)\n",
    "        if epoch < MeanTeacherEpoch:\n",
    "            consistency_loss = 0.0\n",
    "        else:\n",
    "            consistency_loss = torch.mean((soft_out - soft_aug) ** 2) + torch.mean(\n",
    "                (soft_unlabeled - soft_unlabeled_aug) ** 2\n",
    "            )\n",
    "        consistency_weight = get_current_consistency_weight(iter_num // 50)\n",
    "        iter_num += 1\n",
    "        loss = supervised_loss + consistency_weight * consistency_loss\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        update_ema_variables(model, ema_model, 0.99, iter_num)\n",
    "        epoch_loss += loss.item()\n",
    "        print(\n",
    "            # f\"{step}/{len(unlabeled_train_ds) // unlabeled_train_loader.batch_size}, \"\n",
    "            f\"train_loss: {loss.item():.4f}\"\n",
    "        )\n",
    "\n",
    "    epoch_loss /= step\n",
    "    writer.add_scalar(\"Loss/train\", epoch_loss, epoch)\n",
    "    epoch_loss_values.append(epoch_loss)\n",
    "\n",
    "    print(f\"epoch {epoch + 1} average loss: {epoch_loss:.4f}\")\n",
    "\n",
    "    if (epoch + 1) % val_interval == 0:\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for val_data in val_loader:\n",
    "                val_inputs, val_labels = (\n",
    "                    torch.tensor(val_data[0]).to(device),\n",
    "                    torch.tensor(val_data[1]).to(device),\n",
    "                )\n",
    "                val_outputs = model(val_inputs)\n",
    "                val_outputs = [post_pred(i) for i in decollate_batch(val_outputs)]\n",
    "                val_labels = [post_label(i) for i in decollate_batch(val_labels)]\n",
    "                # compute metric for current iteration\n",
    "                dice_metric(y_pred=val_outputs, y=val_labels)\n",
    "\n",
    "            # aggregate the final mean dice result\n",
    "            metric = dice_metric.aggregate().item()\n",
    "            print(f\"val dice: {metric}\")\n",
    "\n",
    "            # reset the status for next validation round\n",
    "            dice_metric.reset()\n",
    "        writer.add_scalar(\"val dice\", metric, epoch)\n",
    "        metric_values.append(metric)\n",
    "        \n",
    "        if metric > best_metric:\n",
    "            best_metric = metric\n",
    "            best_metric_epoch = epoch + 1\n",
    "            torch.save(\n",
    "                model.module.state_dict(),\n",
    "                os.path.join(model_folder, f\"best_{run_time}.pth\"),\n",
    "            )\n",
    "            print(\"saved new best metric model\")\n",
    "        print(\n",
    "            f\"current epoch: {epoch + 1} current mean dice: {metric:.4f}\"\n",
    "            f\"\\nbest mean dice: {best_metric:.4f} \"\n",
    "            f\"at epoch: {best_metric_epoch}\"\n",
    "        )\n",
    "\n",
    "    print(f\"epoch time = {time() - start_time}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "69ce82f0454b58f8538f17de03201ff1941f71155f919ebfc63f1bc59487577f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
