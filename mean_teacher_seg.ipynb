{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from data_loader import data_load\n",
    "from unet_pytorch import build_unet\n",
    "from ramps import get_current_consistency_weight, update_ema_variables\n",
    "from glob import glob\n",
    "import tensorflow as tf\n",
    "from time import time\n",
    "from datetime import datetime\n",
    "from monai.data import decollate_batch\n",
    "from monai.losses import DiceLoss\n",
    "from monai.metrics import DiceMetric\n",
    "from monai.transforms import (\n",
    "    Compose,\n",
    "    AsDiscrete,\n",
    "    EnsureType,\n",
    ")\n",
    "import os\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import rioxarray\n",
    "import numpy as np\n",
    "from sklearn.metrics import average_precision_score\n",
    "from metrics_evaluator import MetricsEvaluator\n",
    "\n",
    "writer = SummaryWriter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(folder, label=True, shuffle_size=500):\n",
    "    features = BANDS + TARGETS if label else BANDS\n",
    "    tf_files = glob(f\"{folder}/*.gz\")\n",
    "    columns = [\n",
    "        tf.io.FixedLenFeature(\n",
    "            shape=KERNEL_SHAPE if label else BUFFERED_SHAPE, dtype=tf.float32\n",
    "        )\n",
    "        for _feature in features\n",
    "    ]\n",
    "    description = dict(zip(features, columns))\n",
    "    data_func = data_load(\n",
    "        tf_files,\n",
    "        BANDS,\n",
    "        description,\n",
    "        response=TARGETS,\n",
    "        shuffle_size=shuffle_size,\n",
    "    )\n",
    "    data = (\n",
    "        data_func.get_training_dataset()\n",
    "        if label\n",
    "        else data_func.get_pridiction_dataset()\n",
    "    )\n",
    "    return data\n",
    "\n",
    "\n",
    "BANDS = [\"blue\", \"green\", \"red\", \"nir\", \"swir1\", \"swir2\", \"ndvi\", \"nirv\"]\n",
    "KERNEL_SHAPE = [256, 256]\n",
    "KERNEL_BUFFER = [128, 128]\n",
    "X_BUFFER, Y_BUFFER = [buffer // 2 for buffer in KERNEL_BUFFER]\n",
    "X_BUFFERED, Y_BUFFERED = (X_BUFFER + KERNEL_SHAPE[0]), (Y_BUFFER + KERNEL_SHAPE[1])\n",
    "BUFFERED_SHAPE = [\n",
    "    kernel + buffer for kernel, buffer in zip(KERNEL_SHAPE, KERNEL_BUFFER)\n",
    "]\n",
    "TARGETS = [\"cropland\"]\n",
    "NCLASS = 2\n",
    "shuffle_size = 500\n",
    "model_folder = f\"/bess23/huaize/semi-supervised/models/\"\n",
    "pred_folder = \"/bess23/huaize/semi-supervised/data/unlabeled\"\n",
    "ground_truth_tiff = glob(pred_folder + \"/*.tif\")[0]\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "batch = 256\n",
    "# batch_test = batch * 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\n",
    "    \"/bess23/huaize/semi-supervised/data/labeled/train/Winter_Wheat_United_States_of_America\",\n",
    "    label=True,\n",
    "    shuffle_size=shuffle_size,\n",
    ")\n",
    "\n",
    "dataset_size = dataset.reduce(0, lambda x, _: x + 1).numpy()\n",
    "# val_dataset = load_dataset(\n",
    "#     batch, \"/bess23/huaize/semi-supervised/data/labeled/valid\", label=True\n",
    "# )\n",
    "validation_size = int(0.3 * dataset_size)  # 0.3\n",
    "train_dataset = dataset.take(validation_size).batch(batch)\n",
    "validation_dataset = dataset.skip(validation_size).batch(batch)\n",
    "test_dataset = load_dataset(pred_folder, label=False).shuffle(shuffle_size).batch(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dice_loss(target, predictive, ep=1e-8):\n",
    "    intersection = 2 * torch.sum(predictive * target) + ep\n",
    "    union = torch.sum(predictive) + torch.sum(target) + ep\n",
    "    loss = 1 - intersection / union\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% train\n",
    "run_time = datetime.today().strftime(\"%m_%d_%H_%M_%S\")\n",
    "max_epochs = 200\n",
    "MeanTeacherEpoch = 20\n",
    "val_interval = 3\n",
    "best_metric = -1\n",
    "best_metric_epoch = -1\n",
    "iter_num = 0\n",
    "epoch_loss_values = []\n",
    "post_pred = Compose([EnsureType(), AsDiscrete(argmax=True, to_onehot=NCLASS)])\n",
    "post_label = Compose([EnsureType(), AsDiscrete(to_onehot=NCLASS)])\n",
    "ckpt_path = os.path.join(model_folder, f\"best_{run_time}.pth\")\n",
    "\n",
    "model = build_unet(len(BANDS), NCLASS).cuda()\n",
    "ema_model = build_unet(len(BANDS), NCLASS).cuda()\n",
    "model = nn.DataParallel(model)\n",
    "ema_model = nn.DataParallel(ema_model)\n",
    "model.to(device)\n",
    "ema_model.to(device)\n",
    "\n",
    "lr = 3e-4\n",
    "opt = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "loss_function = DiceLoss(to_onehot_y=True, softmax=False)\n",
    "dice_metric = DiceMetric(include_background=False, reduction=\"mean\")\n",
    "for epoch in range(max_epochs):\n",
    "    print(\"-\" * 10)\n",
    "    print(f\"epoch {epoch + 1}/{max_epochs}\")\n",
    "    start_time = time()\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    step = 0\n",
    "    train_loader = train_dataset.as_numpy_iterator()\n",
    "    val_loader = validation_dataset.as_numpy_iterator()\n",
    "    unlabeled_train_loader = test_dataset.as_numpy_iterator()\n",
    "    for labeled_batch, unlabeled_batch in zip(train_loader, unlabeled_train_loader):\n",
    "        step += 1\n",
    "        labeled_inputs, labels = (\n",
    "            torch.tensor(labeled_batch[0]).to(device),\n",
    "            torch.tensor(labeled_batch[1]).to(device),\n",
    "        )\n",
    "        unlabeled_batch = unlabeled_batch[\n",
    "            slice(None), slice(None), X_BUFFER:X_BUFFERED, Y_BUFFER:Y_BUFFERED\n",
    "        ]\n",
    "        unlabeled_inputs = torch.tensor(unlabeled_batch).to(device)\n",
    "        opt.zero_grad()\n",
    "        noise_labeled = torch.clamp(torch.randn_like(labeled_inputs) * 0.1, -0.2, 0.2)\n",
    "        noise_unlabeled = torch.clamp(\n",
    "            torch.randn_like(unlabeled_inputs) * 0.1, -0.2, 0.2\n",
    "        )\n",
    "        noise_labeled_inputs = labeled_inputs + noise_labeled\n",
    "        noise_unlabeled_inputs = unlabeled_inputs + noise_unlabeled\n",
    "\n",
    "        outputs = model(labeled_inputs)\n",
    "        with torch.no_grad():\n",
    "            # soft_out = torch.softmax(outputs, dim=1)\n",
    "            outputs_unlabeled = model(unlabeled_inputs)\n",
    "            # soft_unlabeled = torch.softmax(outputs_unlabeled, dim=1)\n",
    "            outputs_aug = ema_model(noise_labeled_inputs)\n",
    "            # soft_aug = torch.softmax(outputs_aug, dim=1)\n",
    "            outputs_unlabeled_aug = ema_model(noise_unlabeled_inputs)\n",
    "            # soft_unlabeled_aug = torch.softmax(outputs_unlabeled_aug, dim=1)\n",
    "\n",
    "        supervised_loss = loss_function(outputs, labels)\n",
    "        if epoch < MeanTeacherEpoch:\n",
    "            consistency_loss = 0.0\n",
    "        else:\n",
    "            consistency_loss = torch.mean(dice_loss(outputs, outputs_aug)) + torch.mean(\n",
    "                dice_loss(outputs_unlabeled, outputs_unlabeled_aug)\n",
    "            )\n",
    "        consistency_weight = get_current_consistency_weight(epoch, max_epochs)\n",
    "        print(\n",
    "            \"consistency_weight is : {}, consistency_loss is: {}\".format(\n",
    "                consistency_weight, consistency_loss\n",
    "            )\n",
    "        )\n",
    "        iter_num += 1\n",
    "        loss = supervised_loss + consistency_weight * consistency_loss\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        update_ema_variables(model, ema_model, 0.99, iter_num)\n",
    "        epoch_loss += loss.item()\n",
    "        print(\n",
    "            # f\"{step}/{len(unlabeled_train_ds) // unlabeled_train_loader.batch_size}, \"\n",
    "            f\"train_loss: {loss.item():.4f}\"\n",
    "        )\n",
    "\n",
    "    epoch_loss /= step\n",
    "    writer.add_scalar(\"Loss/train\", epoch_loss, epoch)\n",
    "    writer.add_scalar(\"supervised_loss/train\", supervised_loss, epoch)\n",
    "    writer.add_scalar(\"consistency_loss/train\", consistency_loss, epoch)\n",
    "    epoch_loss_values.append(epoch_loss)\n",
    "\n",
    "    print(f\"epoch {epoch + 1} average loss: {epoch_loss:.4f}\")\n",
    "\n",
    "    if (epoch + 1) % val_interval == 0:\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_metric = 0\n",
    "            metric_values = []\n",
    "            for val_data in val_loader:\n",
    "                val_inputs, val_labels = (\n",
    "                    torch.tensor(val_data[0]).to(device),\n",
    "                    torch.tensor(val_data[1]).to(device),\n",
    "                )\n",
    "                val_outputs = model(val_inputs)\n",
    "                # print(val_outputs.shape)\n",
    "                # val_outputs = [post_pred(i) for i in decollate_batch(val_outputs)]\n",
    "                # val_labels = [post_label(i) for i in decollate_batch(val_labels)]\n",
    "                # compute metric for current iteration\n",
    "                val_metric = loss_function(val_outputs, val_labels)\n",
    "                metric_values.append(val_metric)\n",
    "\n",
    "            # aggregate the final mean dice result\n",
    "            metric =1- torch.mean(torch.tensor(metric_values)).numpy()\n",
    "            print(f\"val dice: {metric}\")\n",
    "            writer.add_scalar(\"val dice\", metric, epoch)\n",
    "            # reset the status for next validation round\n",
    "            # dice_metric.reset()\n",
    "\n",
    "        if metric > best_metric:\n",
    "            best_metric = metric\n",
    "            best_metric_epoch = epoch + 1\n",
    "            torch.save(\n",
    "                model.module.state_dict(),\n",
    "                ckpt_path,\n",
    "            )\n",
    "            print(\"saved new best metric model\")\n",
    "        print(\n",
    "            f\"current epoch: {epoch + 1} current mean dice: {metric:.4f}\"\n",
    "            f\"\\nbest mean dice: {best_metric:.4f} \"\n",
    "            f\"at epoch: {best_metric_epoch}\"\n",
    "        )\n",
    "\n",
    "    print(f\"epoch time = {time() - start_time}\")\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt_path = ckpt_path\n",
    "tif_filename = \"/bess23/huaize/semi-supervised/tif/{}.tif\".format(\n",
    "    ckpt_path.split(\"/\")[-1]\n",
    ")\n",
    "!python /bess23/huaize/semi-supervised/meanteacherseg/toimage.py --ckpt_path {ckpt_path} --pred_folder {pred_folder} --tif_filename {tif_filename}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_data = rioxarray.open_rasterio(ground_truth_tiff)\n",
    "gt_data = gt_data.where(gt_data != gt_data.rio.nodata)\n",
    "pred_data = rioxarray.open_rasterio(tif_filename)\n",
    "pred_data = pred_data.where(pred_data != pred_data.rio.nodata)\n",
    "pred_data = pred_data.rio.reproject_match(gt_data, nodata=np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of MetricsEvaluator\n",
    "\n",
    "evaluator = MetricsEvaluator(gt_data, pred_data)\n",
    "\n",
    "# Compute F1 score\n",
    "precision, recall, f1, overlay_f1_score = evaluator.f1_score()\n",
    "print(\"Precision:\", round(float(precision.data), 3))\n",
    "print(\"Recall:\", round(float(recall.data), 3))\n",
    "print(\"F1 Score:\", round(float(f1.data), 3))\n",
    "print(\"Overlay F1 Score:\", round(float(overlay_f1_score.data), 3))\n",
    "\n",
    "# Compute Dice coefficient\n",
    "dice_coefficient = evaluator.dice_coefficient()\n",
    "print(\"Dice Coefficient:\", round(float(dice_coefficient), 3))\n",
    "\n",
    "# Compute average precision\n",
    "ap = evaluator.average_precision()\n",
    "print(\"Average Precision:\", round(float(ap), 3))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "eb801671d0fd8279605d132916b99b03f85cbd24e2972767e33d25d60e19f854"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
