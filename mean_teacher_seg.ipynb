{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from data_loader import data_load\n",
    "from att2d_unet import build_unet\n",
    "from ramps import get_current_consistency_weight, update_ema_variables\n",
    "from glob import glob\n",
    "import tensorflow as tf\n",
    "from time import time\n",
    "from datetime import datetime\n",
    "from monai.data import decollate_batch\n",
    "from monai.losses import DiceLoss\n",
    "from monai.metrics import DiceMetric\n",
    "from monai.transforms import (\n",
    "    Compose,\n",
    "    AsDiscrete,\n",
    "    EnsureType,\n",
    ")\n",
    "import os\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import rioxarray\n",
    "import numpy as np\n",
    "from sklearn.metrics import average_precision_score\n",
    "from metrics_evaluator import MetricsEvaluator\n",
    "\n",
    "writer = SummaryWriter()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(folder, label=True, shuffle_size=500):\n",
    "    features = BANDS + TARGETS if label else BANDS\n",
    "    tf_files = glob(f\"{folder}/*.gz\")\n",
    "    columns = [\n",
    "        tf.io.FixedLenFeature(\n",
    "            shape=KERNEL_SHAPE if label else BUFFERED_SHAPE, dtype=tf.float32\n",
    "        )\n",
    "        for _feature in features\n",
    "    ]\n",
    "    description = dict(zip(features, columns))\n",
    "    data_func = data_load(\n",
    "        tf_files,\n",
    "        BANDS,\n",
    "        description,\n",
    "        response=TARGETS,\n",
    "        shuffle_size=shuffle_size,\n",
    "    )\n",
    "    data = (\n",
    "        data_func.get_training_dataset()\n",
    "        if label\n",
    "        else data_func.get_pridiction_dataset()\n",
    "    )\n",
    "    return data\n",
    "\n",
    "\n",
    "BANDS = [\"blue\", \"green\", \"red\", \"nir\", \"swir1\", \"swir2\", \"ndvi\", \"nirv\"]\n",
    "KERNEL_SHAPE = [256, 256]\n",
    "KERNEL_BUFFER = [128, 128]\n",
    "X_BUFFER, Y_BUFFER = [buffer // 2 for buffer in KERNEL_BUFFER]\n",
    "X_BUFFERED, Y_BUFFERED = (X_BUFFER + KERNEL_SHAPE[0]), (Y_BUFFER + KERNEL_SHAPE[1])\n",
    "BUFFERED_SHAPE = [\n",
    "    kernel + buffer for kernel, buffer in zip(KERNEL_SHAPE, KERNEL_BUFFER)\n",
    "]\n",
    "TARGETS = [\"cropland\"]\n",
    "NCLASS = 2\n",
    "shuffle_size = 500\n",
    "model_folder = f\"/bess23/huaize/semi-supervised/models/\"\n",
    "pred_folder = (\n",
    "    \"/bess23/huaize/semi-supervised/data/unlabeled/Colorado/earthengine_Colorado\"\n",
    ")\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "batch = 128\n",
    "# batch_test = batch * 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def power_jaccard_loss(y_true, y_pred, power=2):\n",
    "    '''\n",
    "    Jaccard loss with power\n",
    "    This loss is used in the paper \"A Review on Deep Learning Techniques Applied to Semantic Segmentation\"\n",
    "    Input:\n",
    "        y_true: ground truth\n",
    "        y_pred: prediction\n",
    "        power: power of the jaccard loss\n",
    "    Output:\n",
    "        loss: jaccard loss\n",
    "    '''\n",
    "    y_true = tf.cast(y_true, tf.float32)\n",
    "    y_pred = tf.cast(y_pred, tf.float32)\n",
    "    intersection = tf.reduce_sum(y_true * y_pred, axis=[1, 2, 3])\n",
    "    union = tf.reduce_sum(y_true + y_pred, axis=[1, 2, 3])\n",
    "    return 1 - tf.reduce_mean((intersection + 1) / (union - intersection + 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class dice_loss(nn.Module):\n",
    "    def __init__(self, class_weights=None):\n",
    "        super(dice_loss, self).__init__()\n",
    "        self.class_weights = class_weights\n",
    "\n",
    "    def forward(self, prediction, target, to_onehot_y=False):\n",
    "        epsilon=torch.tensor(1e-8).to(target.device)\n",
    "        if to_onehot_y:\n",
    "            target = target.long()\n",
    "            target_onehot = torch.zeros_like(prediction)\n",
    "            target_onehot.scatter_(1, target, 1)\n",
    "            target = target_onehot\n",
    "            \n",
    "        if self.class_weights is None:\n",
    "            self.class_weights = torch.ones(target.shape[1]).to(target.device)\n",
    "        intersection = 2 * torch.sum(prediction * target, dim=(0, 2, 3)) + epsilon\n",
    "        \n",
    "        union = (\n",
    "            torch.sum(prediction, dim=(0, 2, 3))\n",
    "            + torch.sum(target, dim=(0, 2, 3))\n",
    "            + epsilon\n",
    "        )\n",
    "        class_dice_scores = intersection / union\n",
    "        weighted_dice_scores = class_dice_scores * self.class_weights\n",
    "        loss = 1 - torch.mean(weighted_dice_scores)\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\n",
    "    \"/bess23/huaize/semi-supervised/data/labeled/train\",\n",
    "    label=True,\n",
    "    shuffle_size=shuffle_size,\n",
    ")\n",
    "\n",
    "\n",
    "dataset_size = 27727#dataset.reduce(0, lambda x, _: x + 1).numpy()\n",
    "validation_size = int(0.3 * dataset_size)  # 0.3\n",
    "train_dataset = dataset.take(validation_size).batch(batch)\n",
    "validation_dataset = dataset.skip(validation_size).batch(batch)\n",
    "test_dataset = load_dataset(pred_folder, label=False).shuffle(shuffle_size).batch(batch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% train\n",
    "run_time = datetime.today().strftime(\"%m_%d_%H_%M_%S\")\n",
    "!git checkout -b experiment_{run_time}\n",
    "max_epochs = 200\n",
    "MeanTeacherEpoch = 80\n",
    "val_interval = 1\n",
    "best_metric = -1\n",
    "best_metric_epoch = -1\n",
    "iter_num = 0\n",
    "epoch_loss_values = []\n",
    "post_pred = Compose([EnsureType(), AsDiscrete(argmax=True, to_onehot=NCLASS)])\n",
    "post_label = Compose([EnsureType(), AsDiscrete(to_onehot=NCLASS)])\n",
    "ckpt_path = os.path.join(model_folder, f\"best_{run_time}.pth\")\n",
    "class_weights =  None #torch.tensor([1,1]).to(device)\n",
    "criterion = dice_loss(class_weights=class_weights)\n",
    "model = build_unet(len(BANDS), NCLASS).cuda()\n",
    "ema_model = build_unet(len(BANDS), NCLASS).cuda()\n",
    "model = nn.DataParallel(model)\n",
    "ema_model = nn.DataParallel(ema_model)\n",
    "model.to(device)\n",
    "ema_model.to(device)\n",
    "att_type = \"channel\" # 'channel', 'spatial', 'channel_spatial\n",
    "lr = 0.001\n",
    "opt = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "# scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(opt, mode='min', factor=0.1, patience=10, verbose=True)\n",
    "dice_metric = DiceMetric(include_background=False, reduction=\"mean\")\n",
    "for epoch in range(max_epochs):\n",
    "    print(\"-\" * 10)\n",
    "    print(f\"epoch {epoch + 1}/{max_epochs}\")\n",
    "    start_time = time()\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    step = 0\n",
    "    train_loader = train_dataset.as_numpy_iterator()\n",
    "    val_loader = validation_dataset.as_numpy_iterator()\n",
    "    unlabeled_train_loader = test_dataset.as_numpy_iterator()\n",
    "    for labeled_batch, unlabeled_batch in zip(train_loader, unlabeled_train_loader):\n",
    "        step += 1\n",
    "        labeled_inputs, labels = (\n",
    "            torch.tensor(labeled_batch[0]).to(device),\n",
    "            torch.tensor(labeled_batch[1]).to(device),\n",
    "        )\n",
    "        unlabeled_batch = unlabeled_batch[\n",
    "            slice(None), slice(None), X_BUFFER:X_BUFFERED, Y_BUFFER:Y_BUFFERED\n",
    "        ]\n",
    "        unlabeled_inputs = torch.tensor(unlabeled_batch).to(device)\n",
    "        opt.zero_grad()\n",
    "        noise_labeled = torch.clamp(torch.randn_like(labeled_inputs) * 0.1, -0.2, 0.2)\n",
    "        noise_unlabeled = torch.clamp(\n",
    "            torch.randn_like(unlabeled_inputs) * 0.1, -0.2, 0.2\n",
    "        )\n",
    "        noise_labeled_inputs = labeled_inputs + noise_labeled\n",
    "        noise_unlabeled_inputs = unlabeled_inputs + noise_unlabeled\n",
    "\n",
    "        outputs = model(labeled_inputs)\n",
    "        with torch.no_grad():\n",
    "            outputs_unlabeled = model(unlabeled_inputs)\n",
    "            outputs_aug = ema_model(noise_labeled_inputs)\n",
    "            outputs_unlabeled_aug = ema_model(noise_unlabeled_inputs)\n",
    "\n",
    "        supervised_loss = criterion(outputs, labels, to_onehot_y=True)\n",
    "        if epoch < MeanTeacherEpoch:\n",
    "            consistency_loss = 0.0\n",
    "        else:\n",
    "            consistency_loss = torch.mean(\n",
    "                criterion(outputs, outputs_aug, class_weights)\n",
    "            ) + torch.mean(\n",
    "                criterion(outputs_unlabeled, outputs_unlabeled_aug, class_weights)\n",
    "            )\n",
    "        consistency_weight = get_current_consistency_weight(\n",
    "            (epoch - MeanTeacherEpoch), (max_epochs - MeanTeacherEpoch)\n",
    "        )\n",
    "        print(\n",
    "            \"consistency_weight is : {}, consistency_loss is: {}\".format(\n",
    "                consistency_weight, consistency_loss\n",
    "            )\n",
    "        )\n",
    "        iter_num += 1\n",
    "        loss = supervised_loss + consistency_weight * consistency_loss\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        update_ema_variables(model, ema_model, 0.99, iter_num)\n",
    "        epoch_loss += loss.item()\n",
    "        print(\n",
    "            # f\"{step}/{len(unlabeled_train_ds) // unlabeled_train_loader.batch_size}, \"\n",
    "            f\"train_loss: {loss.item():.4f}\"\n",
    "        )\n",
    "\n",
    "    epoch_loss /= step\n",
    "    writer.add_scalar(\"Loss/train\", epoch_loss, epoch)\n",
    "    writer.add_scalar(\"supervised_loss/train\", supervised_loss, epoch)\n",
    "    writer.add_scalar(\"consistency_loss/train\", consistency_loss, epoch)\n",
    "    epoch_loss_values.append(epoch_loss)\n",
    "\n",
    "    print(f\"epoch {epoch + 1} average loss: {epoch_loss:.4f}\")\n",
    "\n",
    "    if (epoch + 1) % val_interval == 0:\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_metric = 0\n",
    "            metric_values = []\n",
    "            for val_data in val_loader:\n",
    "                val_inputs, val_labels = (\n",
    "                    torch.tensor(val_data[0]).to(device),\n",
    "                    torch.tensor(val_data[1]).to(device),\n",
    "                )\n",
    "                val_outputs = model(val_inputs)\n",
    "                val_metric = criterion(val_outputs, val_labels, to_onehot_y=True)\n",
    "                metric_values.append(val_metric)\n",
    "\n",
    "            # aggregate the final mean dice result\n",
    "            val_loss = torch.mean(torch.tensor(metric_values)).numpy()\n",
    "            metric = 1 - loss\n",
    "            print(f\"val dice: {metric}\")\n",
    "            writer.add_scalar(\"val dice\", metric, epoch)\n",
    "            # reset the status for next validation round\n",
    "            # dice_metric.reset()\n",
    "\n",
    "        if metric > best_metric:\n",
    "            best_metric = metric\n",
    "            best_metric_epoch = epoch + 1\n",
    "            torch.save(\n",
    "                model.module.state_dict(),\n",
    "                ckpt_path,\n",
    "            )\n",
    "            print(\"saved new best metric model\")\n",
    "        print(\n",
    "            f\"current epoch: {epoch + 1} current mean dice: {metric:.4f}\"\n",
    "            f\"\\nbest mean dice: {best_metric:.4f} \"\n",
    "            f\"at epoch: {best_metric_epoch}\"\n",
    "        )\n",
    "    # scheduler.step(val_loss)\n",
    "    print(f\"epoch time = {time() - start_time}\")\n",
    "writer.close()\n",
    "!git commit -m \"Ran experiment {ckpt_path} and made changes to code\"\n",
    "!git checkout main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ckpt_path = '/bess23/huaize/semi-supervised/models/best_03_24_02_40_15.pth'\n",
    "tif_filename = \"/bess23/huaize/semi-supervised/tif/{}.tif\".format(\n",
    "    ckpt_path.split(\"/\")[-1]\n",
    ")\n",
    "print(ckpt_path)\n",
    "!python /bess23/huaize/semi-supervised/meanteacherseg/toimage.py --ckpt_path \"{ckpt_path}\" --pred_folder {pred_folder} --tif_filename {tif_filename} --attention_type {att_type}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth_tiff  = 'earthengine/cdl_Winter_Wheat_South_Dakota_2022.tif'  \n",
    "gt_data = rioxarray.open_rasterio(ground_truth_tiff)\n",
    "gt_data = gt_data.where(gt_data != gt_data.rio.nodata)\n",
    "pred_data = rioxarray.open_rasterio(tif_filename)\n",
    "pred_data = pred_data.where(pred_data != pred_data.rio.nodata)\n",
    "pred_data = pred_data.rio.reproject_match(gt_data, nodata=np.nan)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of MetricsEvaluator\n",
    "evaluator = MetricsEvaluator(gt_data, pred_data)\n",
    "\n",
    "# Compute F1 score\n",
    "precision, recall, f1, overlay_f1_score = evaluator.f1_score()\n",
    "print(\"Precision:\", round(float(precision.data), 3))\n",
    "print(\"Recall:\", round(float(recall.data), 3))\n",
    "print(\"F1 Score:\", round(float(f1.data), 3))\n",
    "print(\"Overlay F1 Score:\", round(float(overlay_f1_score.data), 3))\n",
    "\n",
    "# Compute Dice coefficient\n",
    "# dice_coefficient = evaluator.dice_coefficient()\n",
    "# print(\"Dice Coefficient:\", round(float(dice_coefficient), 3))\n",
    "\n",
    "# Compute average precision\n",
    "# ap = evaluator.average_precision()\n",
    "# print(\"Average Precision:\", round(float(ap), 3))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "eb801671d0fd8279605d132916b99b03f85cbd24e2972767e33d25d60e19f854"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
