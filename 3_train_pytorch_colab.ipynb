{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Hydrometeorological-Remote-Sensing/cropmapping/blob/sw_ww_test/3_train_pytorch_colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t4vkspe7xdDA"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount(\"/content/drive\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OunVTa3wqpiQ"
      },
      "outputs": [],
      "source": [
        "!cp / content/drive/MyDrive/RDA/model_and_env/data_loader.py .\n",
        "!cp / content/drive/MyDrive/RDA/model_and_env/unet_pytorch.py .\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tzpml74S0Pc9"
      },
      "outputs": [],
      "source": [
        "!pip install segmentation_models_pytorch - q\n",
        "!pip install affine - q\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import datetime\n",
        "import segmentation_models_pytorch as smp\n",
        "import numpy as np\n",
        "import torch\n",
        "from glob import glob\n",
        "import torch.utils.data\n",
        "from data_loader import data_load\n",
        "import tensorflow as tf\n",
        "from unet_pytorch import build_unet\n",
        "import torch.nn as nn\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from copy import deepcopy\n",
        "import pandas as pd\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import random_split, DataLoader, Dataset\n",
        "\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "import numpy as np\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wen4SPu9xNe2"
      },
      "outputs": [],
      "source": [
        "writer = SummaryWriter()\n",
        "\n",
        "\n",
        "def dataset(bands, description, batch, folder,targets,train=True, buffer_size=6000):\n",
        "    if train:\n",
        "        sub_folder = \"train\"\n",
        "    else:\n",
        "        sub_folder = \"valid\"\n",
        "    tf_files = glob(\n",
        "        folder + \"/{}/*.gz\".format(sub_folder)\n",
        "    )\n",
        "\n",
        "    print(tf_files)\n",
        "    data = data_load(\n",
        "        tf_files,\n",
        "        bands,\n",
        "        description,\n",
        "        batch_size=batch,\n",
        "        response=targets,\n",
        "        buffer_size=buffer_size,\n",
        "    ).get_training_dataset()\n",
        "    return data\n",
        "\n",
        "\n",
        "class StepRunner:\n",
        "    def __init__(self, net, loss_fn, stage=\"train\", metrics_dict=None, optimizer=None):\n",
        "        self.net, self.loss_fn, self.metrics_dict, self.stage = (\n",
        "            net,\n",
        "            loss_fn,\n",
        "            metrics_dict,\n",
        "            stage,\n",
        "        )\n",
        "        self.optimizer = optimizer\n",
        "\n",
        "    def step(self, features, labels):\n",
        "        # loss\n",
        "        preds = self.net(features)\n",
        "        loss = self.loss_fn(preds, labels)\n",
        "\n",
        "        # backward()\n",
        "        if self.optimizer is not None and self.stage == \"train\":\n",
        "            loss.backward()\n",
        "            self.optimizer.step()\n",
        "            self.optimizer.zero_grad()\n",
        "\n",
        "        # metrics\n",
        "        step_metrics = {\n",
        "            self.stage + \"_\" + name: metric_fn(preds, labels).item()\n",
        "            for name, metric_fn in self.metrics_dict.items()\n",
        "        }\n",
        "        return loss.item(), step_metrics\n",
        "\n",
        "    def train_step(self, features, labels):\n",
        "        self.net.train()  # 训练模式, dropout层发生作用\n",
        "        return self.step(features, labels)\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def eval_step(self, features, labels):\n",
        "        self.net.eval()  # 预测模式, dropout层不发生作用\n",
        "        return self.step(features, labels)\n",
        "\n",
        "    def __call__(self, features, labels):\n",
        "        if self.stage == \"train\":\n",
        "            return self.train_step(features, labels)\n",
        "        else:\n",
        "            return self.eval_step(features, labels)\n",
        "\n",
        "\n",
        "class EpochRunner:\n",
        "    def __init__(self, steprunner):\n",
        "        self.steprunner = steprunner\n",
        "        self.stage = steprunner.stage\n",
        "\n",
        "    def __call__(self, dataset,test_dataset):\n",
        "        total_loss, step = 0, 1\n",
        "        loop = enumerate(dataset.as_numpy_iterator())\n",
        "        loop_unsup = enumerate(test_dataset.as_numpy_iterator())\n",
        "        for _, batch in loop:\n",
        "            x1_sup = torch.tensor(batch[0]).to(device)\n",
        "            x2_sup = torch.tensor(batch[0]).to(device)\n",
        "            y = torch.tensor(batch[1]).to(torch.int64).to(device)\n",
        "            test_batch = next(loop_unsup)\n",
        "            x1_unsup = torch.tensor(test_batch[x_buffer : x_buffer + kernel_shape[0]]).to(device)\n",
        "            x2_unsup = torch.tensor(test_batch[y_buffer : y_buffer + kernel_shape[1]]).to(device)\n",
        "            x1_concat = torch.cat([x1_sup, x1_unsup], dim=0)\n",
        "            x2_concat = torch.cat([x2_sup, x2_unsup], dim=0)\n",
        "\n",
        "            loss, step_metrics = self.steprunner(features, labels)\n",
        "            step_log = dict({self.stage + \"_loss\": loss}, **step_metrics)\n",
        "            total_loss += loss\n",
        "            step += 1\n",
        "            print(step_log, flush=True)\n",
        "\n",
        "        epoch_loss = total_loss / step\n",
        "        epoch_metrics = {\n",
        "            self.stage + \"_\" + name: metric_fn.compute().item()\n",
        "            for name, metric_fn in self.steprunner.metrics_dict.items()\n",
        "        }\n",
        "        epoch_log = dict({self.stage + \"_loss\": epoch_loss}, **epoch_metrics)\n",
        "        print(epoch_log)\n",
        "\n",
        "        for name, metric_fn in self.steprunner.metrics_dict.items():\n",
        "            metric_fn.reset()\n",
        "        return epoch_log\n",
        "\n",
        "\n",
        "def printlog(info):\n",
        "    nowtime = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "    print(\"\\n\" + \"==========\" * 4 + \"%s\" % nowtime + \"==========\" * 4)\n",
        "    print(str(info) + \"\\n\")\n",
        "\n",
        "\n",
        "def train_model(\n",
        "    net,\n",
        "    optimizer,\n",
        "    loss_fn,\n",
        "    metrics_dict,\n",
        "    train_data,\n",
        "    val_data=None,\n",
        "    epochs=30,\n",
        "    ckpt_path=None,\n",
        "    patience=5,\n",
        "    monitor=\"val_loss\",\n",
        "    mode=\"min\",\n",
        "):\n",
        "\n",
        "    history = {}\n",
        "\n",
        "    for epoch in range(1, epochs + 1):\n",
        "        printlog(\"Epoch {0} / {1}\".format(epoch, epochs))\n",
        "\n",
        "        # 1，train -------------------------------------------------\n",
        "        train_step_runner = StepRunner(\n",
        "            net=net,\n",
        "            stage=\"train\",\n",
        "            loss_fn=loss_fn,\n",
        "            metrics_dict=deepcopy(metrics_dict),\n",
        "            optimizer=optimizer,\n",
        "        )\n",
        "        train_epoch_runner = EpochRunner(train_step_runner)\n",
        "        train_metrics = train_epoch_runner(train_data)\n",
        "\n",
        "        for name, metric in train_metrics.items():\n",
        "            history[name] = history.get(name, []) + [metric]\n",
        "\n",
        "        writer.add_scalar(\"Loss/train\", history[\"train_loss\"][-1], epoch)\n",
        "        # 2，validate -------------------------------------------------\n",
        "        if val_data:\n",
        "            val_step_runner = StepRunner(\n",
        "                net=net,\n",
        "                stage=\"val\",\n",
        "                loss_fn=loss_fn,\n",
        "                metrics_dict=deepcopy(metrics_dict),\n",
        "            )\n",
        "            val_epoch_runner = EpochRunner(val_step_runner)\n",
        "            with torch.no_grad():\n",
        "                val_metrics = val_epoch_runner(val_data)\n",
        "            val_metrics[\"epoch\"] = epoch\n",
        "            for name, metric in val_metrics.items():\n",
        "                history[name] = history.get(name, []) + [metric]\n",
        "        writer.add_scalar(\"Loss/val\", history[\"val_loss\"][-1], epoch)\n",
        "        # 3，early-stopping -------------------------------------------------\n",
        "        arr_scores = history[monitor]\n",
        "        best_score_idx = (\n",
        "            np.argmax(arr_scores) if mode == \"max\" else np.argmin(arr_scores)\n",
        "        )\n",
        "        if best_score_idx == len(arr_scores) - 1:\n",
        "            torch.save(net.state_dict(), ckpt_path)\n",
        "            print(\n",
        "                \"<<<<<< reach best {0} : {1} >>>>>>\".format(\n",
        "                    monitor, arr_scores[best_score_idx]\n",
        "                )\n",
        "            )\n",
        "        if len(arr_scores) - best_score_idx > patience:\n",
        "            print(\n",
        "                \"<<<<<< {} without improvement in {} epoch, early stopping >>>>>>\".format(\n",
        "                    monitor, patience\n",
        "                )\n",
        "            )\n",
        "            break\n",
        "        net.load_state_dict(torch.load(ckpt_path))\n",
        "\n",
        "    return pd.DataFrame(history)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fdBs6EnVtW0_"
      },
      "outputs": [],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    bands = [\n",
        "        \"blue\",\n",
        "        \"green\",\n",
        "        \"red\",\n",
        "        \"nir\",\n",
        "        \"swir1\",\n",
        "        \"swir2\",\n",
        "        \"ndvi\",\n",
        "        \"nirv\",\n",
        "    ]\n",
        "    targets = [\"cropland\"]\n",
        "    features = bands + targets\n",
        "    ckpt_path = (\n",
        "        \"/content/drive/MyDrive/RDA/model_and_env/sw_ww_test/ww_56_model/checkpoint.pt\"\n",
        "    )\n",
        "    nclass = 2\n",
        "    batch = 32\n",
        "    class_names = [\"Others\", \"Wheat\"]\n",
        "    columns = [\n",
        "        tf.io.FixedLenFeature(shape=[256, 256], dtype=tf.float32) for k in features\n",
        "    ]\n",
        "    description = dict(zip(features, columns))\n",
        "    metrics_dict = {}\n",
        "    folder = \"/content/drive/MyDrive/RDA/training cdl/data\"\n",
        "    training = dataset(bands, description, batch,folder, buffer_size=1000)\n",
        "    valid = dataset(bands, description, batch, folder, train=False, buffer_size=1000)\n",
        "    device = \"cuda\"\n",
        "    loss_fn = smp.losses.DiceLoss(mode=\"multiclass\", classes=[0, 1])\n",
        "    loss_name = \"DiceLoss\"\n",
        "    model = (\n",
        "        build_unet(len(bands), nclass).cuda()\n",
        "        if device == \"cuda\"\n",
        "        else build_unet(len(bands), nclass)\n",
        "    )\n",
        "    for name, layer in model.named_modules():\n",
        "        if isinstance(layer, torch.nn.Conv2d):\n",
        "            print(name, layer)\n",
        "    print(model.outputs)\n",
        "    model = nn.DataParallel(model)\n",
        "    optimizer = torch.optim.Adam(\n",
        "        [\n",
        "            dict(params=model.parameters(), lr=0.0001),\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    train_model(\n",
        "        model,\n",
        "        optimizer,\n",
        "        loss_fn,\n",
        "        metrics_dict,\n",
        "        training,\n",
        "        val_data=valid,\n",
        "        epochs=10,\n",
        "        ckpt_path=ckpt_path,\n",
        "    )\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "include_colab_link": true,
      "machine_shape": "hm",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.10"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
